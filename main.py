# --- main.py ---
"""
SafetyVision-RAG API
AI-Powered Safety Hazard Detection System using Vision-Language Models and RAG
åŸºäºè§†è§‰-è¯­è¨€æ¨¡å‹å’Œæ£€ç´¢å¢å¼ºç”Ÿæˆçš„AIå®‰å…¨éšæ‚£æ£€æµ‹ç³»ç»Ÿ
"""
import base64
import os
import logging
import tempfile
from pathlib import Path
from typing import Annotated, Optional, List, Dict
from contextlib import asynccontextmanager
from datetime import datetime

from fastapi import FastAPI, UploadFile, File, HTTPException, status, Query
from fastapi.responses import JSONResponse
from fastapi.exceptions import ResponseValidationError
from pydantic import BaseModel, Field, ValidationError

from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import JsonOutputParser
from langchain_core.runnables import RunnablePassthrough, chain
from langchain_core.messages import HumanMessage
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_qdrant import QdrantVectorStore
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from qdrant_client import QdrantClient
from qdrant_client.models import Filter, FieldCondition, MatchValue, Distance, VectorParams

# é…ç½®æ—¥å¿— - ä½¿ç”¨æ›´è¯¦ç»†çš„æ ¼å¼
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - [%(filename)s:%(lineno)d] - %(message)s',
    handlers=[
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# --- 1. å®šä¹‰æˆ‘ä»¬æƒ³è¦çš„ JSON è¾“å‡ºç»“æ„ (Pydantic V2) ---
class SafetyViolation(BaseModel):
    """å®‰å…¨è¿è§„è®°å½•æ¨¡å‹"""
    hazard_id: int = Field(description="éšæ‚£çš„å”¯ä¸€ç¼–å·ï¼Œä»1å¼€å§‹", ge=1)
    hazard_description: str = Field(description="ä»å›¾ç‰‡ä¸­è¯†åˆ«åˆ°çš„å…·ä½“éšæ‚£æè¿°", min_length=1)
    recommendations: str = Field(description="é’ˆå¯¹éšæ‚£çš„å…·ä½“æ•´æ”¹å»ºè®®", min_length=1)
    rule_reference: str = Field(description="ä»å‘é‡æ•°æ®åº“ä¸­æ£€ç´¢åˆ°çš„ã€æœ€ç›¸å…³çš„å®‰å…¨è§„èŒƒåŸæ–‡", min_length=1)
    model_config = {
        "json_schema_extra": {
            "examples": [{
                "hazard_id": 1,
                "hazard_description": "ç«ç¾ç°åœºæ— æœ‰æ•ˆéš”ç¦»",
                "recommendations": "ç«‹å³è®¾ç½®è­¦æˆ’çº¿ï¼Œç–æ•£æ— å…³äººå‘˜",
                "rule_reference": "ã€Šæ¶ˆé˜²æ³•ã€‹ç¬¬äºŒåå…«æ¡"
            }]
        }
    }

class SafetyReport(BaseModel):
    """å®‰å…¨æŠ¥å‘Šæ¨¡å‹"""
    report_id: str = Field(description="æŠ¥å‘Šçš„å”¯ä¸€IDï¼Œä¾‹å¦‚ä¸€ä¸ªUUID")
    violations: List[SafetyViolation] = Field(
        description="åœ¨å›¾ç‰‡ä¸­å‘ç°çš„æ‰€æœ‰éšæ‚£åˆ—è¡¨"
    )
    
    model_config = {
        "json_schema_extra": {
            "examples": [{
                "report_id": "uuid-12345678-1234-5678-1234-567812345678",
                "violations": [
                    {
                        "hazard_id": 1,
                        "hazard_description": "ç«ç¾ç°åœºæ— æœ‰æ•ˆéš”ç¦»",
                        "recommendations": "ç«‹å³è®¾ç½®è­¦æˆ’çº¿ï¼Œç–æ•£æ— å…³äººå‘˜",
                        "rule_reference": "ã€Šæ¶ˆé˜²æ³•ã€‹ç¬¬äºŒåå…«æ¡"
                    }
                ]
            }]
        }
    }


# --- æ–‡æ¡£ç®¡ç†ç›¸å…³æ¨¡å‹ ---
class DocumentDetail(BaseModel):
    """å•ä¸ªæ–‡æ¡£å¤„ç†è¯¦æƒ…"""
    filename: str = Field(description="æ–‡ä»¶å")
    status: str = Field(description="å¤„ç†çŠ¶æ€: success/skipped/failed")
    chunks: Optional[int] = Field(default=None, description="ç”Ÿæˆçš„æ–‡æœ¬å—æ•°é‡")
    message: str = Field(description="å¤„ç†æ¶ˆæ¯")


class UploadResponse(BaseModel):
    """æ–‡æ¡£ä¸Šä¼ å“åº”æ¨¡å‹"""
    success: bool = Field(description="æ˜¯å¦æˆåŠŸ")
    message: str = Field(description="å“åº”æ¶ˆæ¯")
    results: Dict = Field(description="ç»Ÿè®¡ç»“æœ")
    details: List[DocumentDetail] = Field(description="æ¯ä¸ªæ–‡æ¡£çš„å¤„ç†è¯¦æƒ…")


class DeleteDetail(BaseModel):
    """å•ä¸ªæ–‡æ¡£åˆ é™¤è¯¦æƒ…"""
    filename: str = Field(description="æ–‡ä»¶å")
    status: str = Field(description="åˆ é™¤çŠ¶æ€: deleted/not_found/failed")
    chunks_removed: Optional[int] = Field(default=None, description="åˆ é™¤çš„æ–‡æœ¬å—æ•°é‡")
    message: str = Field(description="åˆ é™¤æ¶ˆæ¯")


class DeleteResponse(BaseModel):
    """æ–‡æ¡£åˆ é™¤å“åº”æ¨¡å‹"""
    success: bool = Field(description="æ˜¯å¦æˆåŠŸ")
    message: str = Field(description="å“åº”æ¶ˆæ¯")
    results: Dict = Field(description="ç»Ÿè®¡ç»“æœ")
    details: List[DeleteDetail] = Field(description="æ¯ä¸ªæ–‡æ¡£çš„åˆ é™¤è¯¦æƒ…")


class DocumentInfo(BaseModel):
    """æ–‡æ¡£ä¿¡æ¯æ¨¡å‹"""
    filename: str = Field(description="æ–‡ä»¶å")
    chunks_count: int = Field(description="æ–‡æœ¬å—æ•°é‡")
    source: str = Field(description="æ–‡æ¡£æ¥æºè·¯å¾„")




# --- 2. è¿æ¥åˆ°æ‰€æœ‰çš„ vLLM å’Œ Qdrant å®¹å™¨ ---
# ä½¿ç”¨ç¯å¢ƒå˜é‡é…ç½®ï¼Œæé«˜å¯é…ç½®æ€§

# LLM (Qwen-VL)
llm_qwen = ChatOpenAI(
    model_name=os.environ.get("VLLM_MODEL_NAME", "/model/qwen3-vl-4b"),
    api_key="not-needed",  # vLLM ä¸éœ€è¦ API key
    base_url=os.environ.get("VLLM_CHAT_URL", "http://vllm-qwen-vl:8000/v1"),
    temperature=0.1,
    max_tokens=800,  # å¢åŠ åˆ°800ä»¥é¿å…è¾“å‡ºè¢«æˆªæ–­
)

# Embedding (BGE-m3)
embeddings_bge = OpenAIEmbeddings(
    model=os.environ.get("VLLM_EMBED_MODEL", "/model/bge-m3"),
    api_key="not-needed",
    base_url=os.environ.get("VLLM_EMBED_URL", "http://vllm-bge-m3:8000/v1"),
)

# Vector Store (Qdrant)
qdrant_host = os.environ.get("QDRANT_HOST", "qdrant-server")
qdrant_port = int(os.environ.get("QDRANT_PORT", "6333"))
collection_name = os.environ.get("QDRANT_COLLECTION", "rag-test")
qdrant_client = QdrantClient(url=f"http://{qdrant_host}:{qdrant_port}")

# ç¡®ä¿é›†åˆå­˜åœ¨ï¼ˆå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»ºï¼‰
def ensure_collection_exists():
    """ç¡®ä¿ Qdrant é›†åˆå­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»º"""
    try:
        # å°è¯•è·å–é›†åˆä¿¡æ¯
        qdrant_client.get_collection(collection_name=collection_name)
        logger.info(f"âœ… é›†åˆ '{collection_name}' å·²å­˜åœ¨")
    except Exception as e:
        # é›†åˆä¸å­˜åœ¨ï¼Œåˆ›å»ºæ–°é›†åˆ
        logger.warning(f"âš ï¸  é›†åˆ '{collection_name}' ä¸å­˜åœ¨ï¼Œæ­£åœ¨åˆ›å»º...")
        try:
            # åˆ›å»ºé›†åˆï¼ˆä½¿ç”¨ä¸ BGE-m3 åŒ¹é…çš„å‘é‡ç»´åº¦ï¼‰
            qdrant_client.create_collection(
                collection_name=collection_name,
                vectors_config=VectorParams(
                    size=1024,  # BGE-m3 çš„å‘é‡ç»´åº¦
                    distance=Distance.COSINE
                )
            )
            logger.info(f"âœ… æˆåŠŸåˆ›å»ºé›†åˆ '{collection_name}'")
        except Exception as create_error:
            logger.error(f"âŒ åˆ›å»ºé›†åˆå¤±è´¥: {create_error}")
            raise

# å¯åŠ¨æ—¶ç¡®ä¿é›†åˆå­˜åœ¨
ensure_collection_exists()

# åˆå§‹åŒ– Vector Store
vector_store = QdrantVectorStore(
    client=qdrant_client,
    collection_name=collection_name,
    embedding=embeddings_bge,
)

# åˆ›å»º RAG æ£€ç´¢å™¨
retriever = vector_store.as_retriever(
    search_kwargs={
        "k": int(os.environ.get("RAG_TOP_K", "2"))
    }
)


# --- 3. (æ ¸å¿ƒ) å®šä¹‰æˆ‘ä»¬çš„ LangChain "é“¾" (LCEL) ---

# é“¾ 1: VLM é“¾ (å›¾ç‰‡ -> éšæ‚£æ–‡æœ¬) - ä½¿ç”¨ç°ä»£çš„ @chain è£…é¥°å™¨
def create_vlm_prompt(base64_image: str):
    """è¾…åŠ©å‡½æ•°ï¼šå°† base64 å­—ç¬¦ä¸²æ ¼å¼åŒ–ä¸º LangChain çš„å¤šæ¨¡æ€æ¶ˆæ¯"""
    return [
        HumanMessage(
            content=[
                {"type": "text", "text": "ä½ æ˜¯ä¸€ä¸ªå®‰å…¨ä¸“å®¶ã€‚åˆ†æå›¾ç‰‡å¹¶ç®€æ´åˆ—å‡º3-5ä¸ªæœ€é‡è¦çš„å®‰å…¨éšæ‚£ï¼Œæ¯æ¡ç”¨ä¸€è¡Œæè¿°ï¼Œä¸è¦é‡å¤ã€‚"},
                {"type": "image_url", "image_url": {"url": f"data:image/png;base64,{base64_image}"}}
            ]
        )
    ]

@chain
async def vlm_chain(base64_image: str) -> str:
    """VLM é“¾ï¼šå°†å›¾ç‰‡è½¬æ¢ä¸ºéšæ‚£æè¿°æ–‡æœ¬"""
    logger.info("ğŸ” VLM å¼€å§‹åˆ†æå›¾ç‰‡...")
    messages = create_vlm_prompt(base64_image)
    response = await llm_qwen.ainvoke(messages)
    result = response.content if hasattr(response, 'content') else str(response)
    logger.info(f"âœ… VLM åˆ†æå®Œæˆï¼Œè¯†åˆ«åˆ°çš„éšæ‚£:\n{result}")
    return result


# é“¾ 2: RAG + JSON é“¾ (éšæ‚£æ–‡æœ¬ -> JSON æŠ¥å‘Š)
rag_prompt_template = """
ä½ æ˜¯ä¸€ä¸ªå®‰å…¨æŠ¥å‘Šç”Ÿæˆå™¨ã€‚
æ ¹æ®ä»¥ä¸‹æ£€ç´¢åˆ°çš„"ç›¸å…³è§„èŒƒ"å’Œ"å‘ç°çš„éšæ‚£"ï¼Œç”Ÿæˆä¸€ä»½ç»“æ„åŒ–çš„ JSON æŠ¥å‘Šã€‚

ä½ å¿…é¡»ä¸¥æ ¼éµå¾ªä»¥ä¸‹ JSON æ ¼å¼ã€‚

{format_instructions}

---
ç›¸å…³è§„èŒƒ (ä¸Šä¸‹æ–‡):
{context}

---
å‘ç°çš„éšæ‚£ (é—®é¢˜):
{question}
"""

# åˆ›å»º JSON è§£æå™¨
json_parser = JsonOutputParser(pydantic_object=SafetyReport)

# åˆ›å»ºå¸¦æ—¥å¿—çš„ä¸Šä¸‹æ–‡å¤„ç†å‡½æ•°
def format_docs_with_logging(docs):
    """æ ¼å¼åŒ–æ–‡æ¡£å¹¶è®°å½•æ—¥å¿—ï¼Œé™åˆ¶æ¯ä¸ªæ–‡æ¡£æœ€å¤š800å­—ç¬¦"""
    truncated_docs = []
    for d in docs:
        content = d.page_content
        if len(content) > 800:
            content = content[:800] + "..."
        truncated_docs.append(content)
    
    formatted = "\n---\n".join(truncated_docs)
    logger.info(f"ğŸ“š RAG æ£€ç´¢åˆ° {len(docs)} ä¸ªç›¸å…³è§„èŒƒæ–‡æ¡£")
    logger.info(f"æ£€ç´¢å†…å®¹é¢„è§ˆ:\n{formatted[:500]}..." if len(formatted) > 500 else f"æ£€ç´¢å†…å®¹:\n{formatted}")
    return formatted

@chain
async def log_prompt(messages):
    """è®°å½•å‘é€ç»™LLMçš„prompt"""
    full_prompt = str(messages)
    logger.info(f"ğŸ“ å‘é€ç»™VLMçš„prompté•¿åº¦: {len(full_prompt)} å­—ç¬¦")
    logger.info(f"ğŸ“ Promptå‰1500å­—ç¬¦:\n{full_prompt[:1500]}...")
    if len(full_prompt) > 1500:
        logger.info(f"ğŸ“ Promptå500å­—ç¬¦:\n...{full_prompt[-500:]}")
    return messages

@chain
async def log_response(response):
    """è®°å½•LLMçš„å“åº”"""
    response_content = response.content if hasattr(response, 'content') else str(response)
    logger.info(f"ğŸ’¬ VLMè¿”å›çš„åŸå§‹å“åº” (å‰1000å­—ç¬¦):\n{response_content[:1000]}...")
    return response

rag_chain = (
    {
        # "retriever | format_docs" ä¼šè‡ªåŠ¨è¿è¡Œ RAG æ£€ç´¢ï¼Œå¹¶å°†æ–‡æ¡£åˆ—è¡¨æ ¼å¼åŒ–ä¸ºå­—ç¬¦ä¸²
        "context": retriever | format_docs_with_logging,
        # "question" é”®ä¼šæ¥æ”¶ "vlm_chain" çš„è¾“å‡º
        "question": RunnablePassthrough(),
        "format_instructions": lambda x: json_parser.get_format_instructions(),
    }
    | ChatPromptTemplate.from_template(rag_prompt_template)
    | log_prompt
    | llm_qwen  # å†æ¬¡è°ƒç”¨ Qwen-VL (å®ƒä¹Ÿå¯ä»¥å¤„ç†çº¯æ–‡æœ¬)
    | log_response
    | json_parser
)

# æœ€ç»ˆç®¡é“: å°†ä¸¤ä¸ªé“¾"ç²˜åˆ"åœ¨ä¸€èµ·
full_pipeline = vlm_chain | rag_chain


# --- 4. ç”Ÿå‘½å‘¨æœŸç®¡ç† ---
@asynccontextmanager
async def lifespan(app: FastAPI):
    """åº”ç”¨ç”Ÿå‘½å‘¨æœŸç®¡ç†"""
    # Startup
    logger.info("ğŸš€ åº”ç”¨å¯åŠ¨ä¸­...")
    logger.info(f"Qdrant æœåŠ¡: {qdrant_client.get_collections()}")
    logger.info(f"Collection: {os.environ.get('QDRANT_COLLECTION', 'rag-test')}")
    logger.info("âœ… åº”ç”¨å¯åŠ¨å®Œæˆ")
    
    yield
    
    # Shutdown
    logger.info("ğŸ‘‹ åº”ç”¨å…³é—­ä¸­...")
    qdrant_client.close()
    logger.info("âœ… åº”ç”¨å…³é—­å®Œæˆ")


# --- 5. åˆ›å»º FastAPI åº”ç”¨ ---
app = FastAPI(
    title="SafetyVision-RAG API",
    description="AI-Powered Safety Hazard Detection System | åŸºäº VLM + RAG çš„æ™ºèƒ½å®‰å…¨éšæ‚£æ£€æµ‹ç³»ç»Ÿ",
    version="2.0.0",
    lifespan=lifespan,
    docs_url="/docs",
    redoc_url="/redoc",
)


# å¥åº·æ£€æŸ¥ç«¯ç‚¹
@app.get("/health", tags=["Health"])
async def health_check():
    """å¥åº·æ£€æŸ¥ç«¯ç‚¹"""
    try:
        # æ£€æŸ¥ Qdrant è¿æ¥
        collections = qdrant_client.get_collections()
        return {
            "status": "healthy",
            "qdrant": "connected",
            "collections": len(collections.collections)
        }
    except Exception as e:
        logger.error(f"å¥åº·æ£€æŸ¥å¤±è´¥: {e}")
        raise HTTPException(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
            detail=f"æœåŠ¡ä¸å¯ç”¨: {str(e)}"
        )


@app.post("/analyze_image", response_model=SafetyReport, tags=["Analysis"])
async def analyze_image(
    file: Annotated[UploadFile, File(description="éœ€è¦åˆ†æçš„å›¾ç‰‡æ–‡ä»¶")]
):
    """
    åˆ†æå›¾ç‰‡ä¸­çš„å®‰å…¨éšæ‚£
    
    ä¸Šä¼ ä¸€å¼ å›¾ç‰‡ï¼ŒVLM å°†è¯†åˆ«éšæ‚£ï¼ŒRAG å°†æ£€ç´¢ç›¸å…³å®‰å…¨è§„èŒƒï¼Œ
    æœ€åè¿”å›ä¸€ä»½ç»“æ„åŒ–çš„ JSON å®‰å…¨æŠ¥å‘Šã€‚
    
    Args:
        file: ä¸Šä¼ çš„å›¾ç‰‡æ–‡ä»¶ (æ”¯æŒ JPEG, PNG ç­‰æ ¼å¼)
        
    Returns:
        SafetyReport: åŒ…å«éšæ‚£åˆ—è¡¨å’Œè§„èŒƒå¼•ç”¨çš„ç»“æ„åŒ–æŠ¥å‘Š
        
    Raises:
        HTTPException: 
            - 400: æ— æ•ˆçš„æ–‡ä»¶ç±»å‹æˆ–ç©ºæ–‡ä»¶
            - 422: JSON è§£æå¤±è´¥
            - 500: æœåŠ¡å™¨å†…éƒ¨é”™è¯¯
    """
    # è®°å½•è¯·æ±‚ä¿¡æ¯
    logger.info(f"ğŸ“· æ”¶åˆ°å›¾ç‰‡åˆ†æè¯·æ±‚: {file.filename}")
    logger.info(f"ğŸ“ Content-Type: {file.content_type}")
    
    # 1. éªŒè¯æ–‡ä»¶ç±»å‹
    if not file.content_type or not file.content_type.startswith('image/'):
        logger.warning(f"âŒ æ— æ•ˆçš„æ–‡ä»¶ç±»å‹: {file.content_type}")
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail={
                "error": "invalid_content_type",
                "message": f"æ— æ•ˆçš„æ–‡ä»¶ç±»å‹: {file.content_type}",
                "accepted_types": ["image/jpeg", "image/png", "image/jpg", "image/webp"]
            }
        )
    
    try:
        # 2. è¯»å–å¹¶éªŒè¯å›¾ç‰‡
        image_bytes = await file.read()
        if len(image_bytes) == 0:
            logger.warning("âŒ ä¸Šä¼ çš„æ–‡ä»¶ä¸ºç©º")
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail={"error": "empty_file", "message": "ä¸Šä¼ çš„æ–‡ä»¶ä¸ºç©º"}
            )
        
        # æ£€æŸ¥æ–‡ä»¶å¤§å° (é™åˆ¶ä¸º10MB)
        max_size = 10 * 1024 * 1024  # 10MB
        if len(image_bytes) > max_size:
            logger.warning(f"âŒ æ–‡ä»¶è¿‡å¤§: {len(image_bytes)} bytes")
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail={
                    "error": "file_too_large",
                    "message": f"æ–‡ä»¶å¤§å°è¶…è¿‡é™åˆ¶ (æœ€å¤§ {max_size/(1024*1024):.1f}MB)",
                    "size": len(image_bytes)
                }
            )
            
        image_base64 = base64.b64encode(image_bytes).decode("utf-8")
        logger.info(f"âœ… å›¾ç‰‡ç¼–ç å®Œæˆ: {len(image_bytes)} bytes")
        logger.info("="*60)
        logger.info("ğŸš€ å¼€å§‹æ‰§è¡Œå®Œæ•´çš„ RAG åˆ†ææµç¨‹...")
        logger.info("="*60)
        
        # 3. è¿è¡Œ LCEL ç®¡é“
        response_json = await full_pipeline.ainvoke(image_base64)
        
        logger.info("="*60)
        logger.info(f"ğŸ“Š æœ€ç»ˆç”Ÿæˆçš„æŠ¥å‘Š: {response_json}")
        logger.info(f"âœ… åˆ†æå®Œæˆ, å‘ç° {len(response_json.get('violations', []))} ä¸ªéšæ‚£")
        logger.info("="*60)
        
        return response_json
        
    except HTTPException:
        # é‡æ–°æŠ›å‡º HTTP å¼‚å¸¸
        raise
    
    except (ValueError, ValidationError, ResponseValidationError) as e:
        # JSONè§£æå¤±è´¥æˆ–PydanticéªŒè¯å¤±è´¥ï¼ˆé€šå¸¸æ˜¯VLMè¾“å‡ºè¢«æˆªæ–­ï¼‰
        logger.error(f"âŒ æ•°æ®éªŒè¯å¤±è´¥: {type(e).__name__}: {e}", exc_info=True)
        
        # å°è¯•æå–éƒ¨åˆ†ç»“æœ
        error_detail = {
            "error": "validation_error",
            "message": "æ¨¡å‹è¾“å‡ºä¸å®Œæ•´æˆ–æ ¼å¼ä¸ç¬¦åˆè¦æ±‚ï¼Œå¯èƒ½æ˜¯å“åº”è¢«æˆªæ–­",
            "error_type": type(e).__name__,
            "details": str(e),
            "suggestion": "å»ºè®®ï¼šå›¾ç‰‡å¯èƒ½è¿‡äºå¤æ‚æˆ–éšæ‚£è¿‡å¤šï¼Œè¯·å°è¯•æ›´ç®€å•çš„å›¾ç‰‡æˆ–è”ç³»ç®¡ç†å‘˜å¢åŠ æ¨¡å‹è¾“å‡ºé•¿åº¦é™åˆ¶"
        }
        
        raise HTTPException(
            status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,
            detail=error_detail
        )
        
    except Exception as e:
        logger.error(f"âŒ åˆ†æå¤„ç†å¤±è´¥: {e}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail={
                "error": "internal_server_error",
                "message": "æœåŠ¡å™¨å†…éƒ¨é”™è¯¯ï¼Œè¯·ç¨åé‡è¯•",
                "details": str(e)
            }
        )
    finally:
        # æ¸…ç†èµ„æº
        await file.close()


# æ ¹è·¯å¾„é‡å®šå‘åˆ°æ–‡æ¡£
@app.get("/", tags=["Root"])
async def root():
    """æ ¹è·¯å¾„ - è¿”å› API ä¿¡æ¯"""
    return {
        "name": "SafetyVision-RAG",
        "description": "AI-Powered Safety Hazard Detection System",
        "version": "2.0.0",
        "docs": "/docs",
        "health": "/health"
    }


# --- æ–‡æ¡£ç®¡ç†æ¥å£ ---

# é…ç½®å¸¸é‡
MAX_FILE_SIZE = 10 * 1024 * 1024  # 10MB per file
MAX_FILES_COUNT = 10               # ä¸€æ¬¡æœ€å¤š10ä¸ªæ–‡ä»¶
COLLECTION_NAME = os.environ.get("QDRANT_COLLECTION", "rag-test")

# åˆå§‹åŒ–æ–‡æœ¬åˆ†å‰²å™¨
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200,
    length_function=len,
    separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", ".", "!", "?", " ", ""]
)


def is_document_exists(filename: str) -> bool:
    """
    æ£€æŸ¥æ–‡æ¡£æ˜¯å¦å·²å­˜åœ¨äºå‘é‡åº“ä¸­
    
    Args:
        filename: æ–‡ä»¶å
        
    Returns:
        æ˜¯å¦å­˜åœ¨
    """
    try:
        result = qdrant_client.scroll(
            collection_name=COLLECTION_NAME,
            scroll_filter={
                "must": [
                    {"key": "metadata.filename", "match": {"value": filename}}
                ]
            },
            limit=1,
            with_payload=True,
            with_vectors=False
        )
        return len(result[0]) > 0
    except Exception as e:
        logger.error(f"æ£€æŸ¥æ–‡æ¡£å­˜åœ¨æ€§å¤±è´¥: {e}")
        return False


def get_document_chunks_count(filename: str) -> int:
    """
    è·å–æ–‡æ¡£çš„æ–‡æœ¬å—æ•°é‡
    
    Args:
        filename: æ–‡ä»¶å
        
    Returns:
        æ–‡æœ¬å—æ•°é‡
    """
    try:
        result = qdrant_client.scroll(
            collection_name=COLLECTION_NAME,
            scroll_filter={
                "must": [
                    {"key": "metadata.filename", "match": {"value": filename}}
                ]
            },
            limit=10000,  # å‡è®¾å•ä¸ªæ–‡æ¡£ä¸ä¼šè¶…è¿‡10000ä¸ªchunk
            with_payload=False,
            with_vectors=False
        )
        return len(result[0])
    except Exception as e:
        logger.error(f"è·å–æ–‡æ¡£å—æ•°é‡å¤±è´¥: {e}")
        return 0


async def process_pdf_file(
    file: UploadFile,
    skip_existing: bool,
    update_existing: bool
) -> DocumentDetail:
    """
    å¤„ç†å•ä¸ªPDFæ–‡ä»¶
    
    Args:
        file: ä¸Šä¼ çš„æ–‡ä»¶
        skip_existing: æ˜¯å¦è·³è¿‡å·²å­˜åœ¨çš„æ–‡æ¡£
        update_existing: æ˜¯å¦æ›´æ–°å·²å­˜åœ¨çš„æ–‡æ¡£
        
    Returns:
        æ–‡æ¡£å¤„ç†è¯¦æƒ…
    """
    filename = file.filename
    
    try:
        # 1. æ£€æŸ¥æ–‡ä»¶ç±»å‹
        if not filename.lower().endswith('.pdf'):
            return DocumentDetail(
                filename=filename,
                status="failed",
                message="æ–‡ä»¶æ ¼å¼é”™è¯¯ï¼Œåªæ”¯æŒPDFæ–‡ä»¶"
            )
        
        # 2. æ£€æŸ¥æ–‡ä»¶å¤§å°
        content = await file.read()
        if len(content) > MAX_FILE_SIZE:
            return DocumentDetail(
                filename=filename,
                status="failed",
                message=f"æ–‡ä»¶è¿‡å¤§ï¼Œè¶…è¿‡{MAX_FILE_SIZE/(1024*1024):.1f}MBé™åˆ¶"
            )
        
        # 3. æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
        exists = is_document_exists(filename)
        if exists:
            if skip_existing and not update_existing:
                logger.info(f"â­ï¸  è·³è¿‡å·²å­˜åœ¨çš„æ–‡æ¡£: {filename}")
                return DocumentDetail(
                    filename=filename,
                    status="skipped",
                    message="æ–‡æ¡£å·²å­˜åœ¨ï¼Œå·²è·³è¿‡"
                )
            elif update_existing:
                logger.info(f"ğŸ”„ æ›´æ–°æ–‡æ¡£: {filename}")
                # å…ˆåˆ é™¤æ—§æ–‡æ¡£
                delete_result = qdrant_client.delete(
                    collection_name=COLLECTION_NAME,
                    points_selector=Filter(
                        must=[
                            FieldCondition(
                                key="metadata.filename",
                                match=MatchValue(value=filename)
                            )
                        ]
                    )
                )
        
        # 4. ä¿å­˜ä¸´æ—¶æ–‡ä»¶
        with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp_file:
            tmp_file.write(content)
            tmp_path = tmp_file.name
        
        try:
            # 5. åŠ è½½PDF
            logger.info(f"ğŸ“„ åŠ è½½PDF: {filename}")
            loader = PyPDFLoader(tmp_path)
            documents = loader.load()
            
            # æ·»åŠ å…ƒæ•°æ®
            for doc in documents:
                doc.metadata.update({
                    "source": filename,
                    "filename": filename,
                    "upload_time": datetime.now().isoformat()
                })
            
            # 6. åˆ†å‰²æ–‡æ¡£
            logger.info(f"âœ‚ï¸  åˆ†å‰²æ–‡æ¡£: {filename}")
            chunks = text_splitter.split_documents(documents)
            
            if not chunks:
                return DocumentDetail(
                    filename=filename,
                    status="failed",
                    message="æ–‡æ¡£åˆ†å‰²å¤±è´¥ï¼Œæœªç”Ÿæˆæ–‡æœ¬å—"
                )
            
            # 7. å‘é‡åŒ–å¹¶å­˜å‚¨
            logger.info(f"ğŸ’¾ å­˜å‚¨æ–‡æ¡£: {filename}, å…± {len(chunks)} ä¸ªæ–‡æœ¬å—")
            QdrantVectorStore.from_documents(
                documents=chunks,
                embedding=embeddings_bge,
                url=f"http://{qdrant_host}:{qdrant_port}",
                collection_name=COLLECTION_NAME,
                force_recreate=False  # è¿½åŠ æ¨¡å¼
            )
            
            logger.info(f"âœ… æ–‡æ¡£å¤„ç†æˆåŠŸ: {filename}")
            return DocumentDetail(
                filename=filename,
                status="success",
                chunks=len(chunks),
                message="æ–‡æ¡£ä¸Šä¼ æˆåŠŸ" if not exists else "æ–‡æ¡£æ›´æ–°æˆåŠŸ"
            )
            
        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            Path(tmp_path).unlink(missing_ok=True)
    
    except Exception as e:
        logger.error(f"âŒ å¤„ç†æ–‡æ¡£å¤±è´¥ {filename}: {e}", exc_info=True)
        return DocumentDetail(
            filename=filename,
            status="failed",
            message=f"å¤„ç†å¤±è´¥: {str(e)}"
        )


@app.post("/api/documents/upload", response_model=UploadResponse, tags=["Documents"])
async def upload_documents(
    files: List[UploadFile] = File(..., description="PDFæ–‡ä»¶åˆ—è¡¨ï¼Œæ”¯æŒæ‰¹é‡ä¸Šä¼ "),
    skip_existing: bool = Query(True, description="æ˜¯å¦è·³è¿‡å·²å­˜åœ¨çš„æ–‡æ¡£"),
    update_existing: bool = Query(False, description="æ˜¯å¦æ›´æ–°å·²å­˜åœ¨çš„æ–‡æ¡£")
) -> UploadResponse:
    """
    ä¸Šä¼ å•ä¸ªæˆ–å¤šä¸ªPDFæ–‡æ¡£åˆ°å‘é‡åº“ï¼ˆè¿½åŠ æ¨¡å¼ï¼‰
    
    åŠŸèƒ½è¯´æ˜ï¼š
    - æ”¯æŒæ‰¹é‡ä¸Šä¼ ï¼ˆæœ€å¤š10ä¸ªæ–‡ä»¶ï¼‰
    - è‡ªåŠ¨æ£€æµ‹é‡å¤æ–‡æ¡£
    - å¯é€‰æ‹©è·³è¿‡æˆ–æ›´æ–°å·²å­˜åœ¨çš„æ–‡æ¡£
    - è¿”å›æ¯ä¸ªæ–‡æ¡£çš„è¯¦ç»†å¤„ç†ç»“æœ
    
    Args:
        files: PDFæ–‡ä»¶åˆ—è¡¨
        skip_existing: è·³è¿‡å·²å­˜åœ¨çš„æ–‡æ¡£ï¼ˆé»˜è®¤Trueï¼‰
        update_existing: æ›´æ–°å·²å­˜åœ¨çš„æ–‡æ¡£ï¼ˆé»˜è®¤Falseï¼‰
                        æ³¨æ„ï¼šupdate_existing=Trueæ—¶ä¼šè¦†ç›–skip_existing
        
    Returns:
        ä¸Šä¼ ç»“æœï¼ŒåŒ…å«ç»Ÿè®¡ä¿¡æ¯å’Œæ¯ä¸ªæ–‡æ¡£çš„è¯¦ç»†çŠ¶æ€
        
    Raises:
        HTTPException: 
            - 400: æ–‡ä»¶æ•°é‡è¶…é™æˆ–æ–‡ä»¶ç±»å‹é”™è¯¯
            - 500: æœåŠ¡å™¨å¤„ç†é”™è¯¯
    
    ç¤ºä¾‹ï¼š
        å•ä¸ªæ–‡ä»¶ä¸Šä¼ ï¼š
        curl -X POST "http://localhost:8080/api/documents/upload" \\
             -F "files=@document.pdf"
        
        æ‰¹é‡ä¸Šä¼ ï¼š
        curl -X POST "http://localhost:8080/api/documents/upload" \\
             -F "files=@doc1.pdf" \\
             -F "files=@doc2.pdf"
        
        å¼ºåˆ¶æ›´æ–°ï¼š
        curl -X POST "http://localhost:8080/api/documents/upload?update_existing=true" \\
             -F "files=@document.pdf"
    """
    logger.info(f"ğŸ“¤ æ”¶åˆ°æ–‡æ¡£ä¸Šä¼ è¯·æ±‚ï¼Œæ–‡ä»¶æ•°: {len(files)}")
    
    # 1. éªŒè¯æ–‡ä»¶æ•°é‡
    if len(files) > MAX_FILES_COUNT:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail={
                "error": "too_many_files",
                "message": f"æ–‡ä»¶æ•°é‡è¶…é™ï¼Œæœ€å¤šæ”¯æŒ{MAX_FILES_COUNT}ä¸ªæ–‡ä»¶",
                "received": len(files),
                "limit": MAX_FILES_COUNT
            }
        )
    
    # 2. éªŒè¯è‡³å°‘æœ‰ä¸€ä¸ªæ–‡ä»¶
    if not files:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail={
                "error": "no_files",
                "message": "æœªæ¥æ”¶åˆ°ä»»ä½•æ–‡ä»¶"
            }
        )
    
    # 3. å¤„ç†æ¯ä¸ªæ–‡ä»¶
    details = []
    processed = 0
    skipped = 0
    updated = 0
    failed = 0
    total_chunks = 0
    
    for file in files:
        detail = await process_pdf_file(file, skip_existing, update_existing)
        details.append(detail)
        
        if detail.status == "success":
            if is_document_exists(detail.filename) and update_existing:
                updated += 1
            else:
                processed += 1
            total_chunks += detail.chunks or 0
        elif detail.status == "skipped":
            skipped += 1
        else:
            failed += 1
    
    # 4. æ„å»ºå“åº”
    success = failed == 0
    results = {
        "processed": processed,
        "skipped": skipped,
        "updated": updated,
        "failed": failed,
        "total_chunks": total_chunks,
        "total_files": len(files)
    }
    
    logger.info("="*60)
    logger.info(f"ğŸ“Š ä¸Šä¼ ç»Ÿè®¡: {results}")
    logger.info("="*60)
    
    return UploadResponse(
        success=success,
        message="æ–‡æ¡£ä¸Šä¼ å®Œæˆ" if success else "éƒ¨åˆ†æ–‡æ¡£ä¸Šä¼ å¤±è´¥",
        results=results,
        details=details
    )


@app.delete("/api/documents", response_model=DeleteResponse, tags=["Documents"])
async def delete_documents(
    filenames: List[str] = Query(..., description="è¦åˆ é™¤çš„æ–‡ä»¶ååˆ—è¡¨ï¼Œå¦‚: ['doc1.pdf', 'doc2.pdf']"),
    force: bool = Query(False, description="å¼ºåˆ¶åˆ é™¤ï¼ˆå³ä½¿æ–‡æ¡£ä¸å­˜åœ¨ä¹Ÿä¸æŠ¥é”™ï¼‰")
) -> DeleteResponse:
    """
    åˆ é™¤å•ä¸ªæˆ–å¤šä¸ªæ–‡æ¡£
    
    åŠŸèƒ½è¯´æ˜ï¼š
    - æ”¯æŒæ‰¹é‡åˆ é™¤
    - ä»å‘é‡åº“ä¸­å®Œå…¨ç§»é™¤æ–‡æ¡£çš„æ‰€æœ‰æ–‡æœ¬å—
    - è¿”å›æ¯ä¸ªæ–‡æ¡£çš„è¯¦ç»†åˆ é™¤ç»“æœ
    
    Args:
        filenames: æ–‡ä»¶ååˆ—è¡¨
        force: å¼ºåˆ¶åˆ é™¤æ¨¡å¼ï¼Œä¸å­˜åœ¨çš„æ–‡æ¡£ä¹Ÿè¿”å›æˆåŠŸ
        
    Returns:
        åˆ é™¤ç»“æœï¼ŒåŒ…å«ç»Ÿè®¡ä¿¡æ¯å’Œæ¯ä¸ªæ–‡æ¡£çš„è¯¦ç»†çŠ¶æ€
        
    Raises:
        HTTPException: 
            - 400: å‚æ•°é”™è¯¯
            - 500: æœåŠ¡å™¨å¤„ç†é”™è¯¯
    
    ç¤ºä¾‹ï¼š
        åˆ é™¤å•ä¸ªæ–‡æ¡£ï¼š
        curl -X DELETE "http://localhost:8080/api/documents?filenames=doc.pdf"
        
        åˆ é™¤å¤šä¸ªæ–‡æ¡£ï¼š
        curl -X DELETE "http://localhost:8080/api/documents?filenames=doc1.pdf&filenames=doc2.pdf"
        
        å¼ºåˆ¶åˆ é™¤ï¼š
        curl -X DELETE "http://localhost:8080/api/documents?filenames=doc.pdf&force=true"
    """
    logger.info(f"ğŸ—‘ï¸  æ”¶åˆ°æ–‡æ¡£åˆ é™¤è¯·æ±‚ï¼Œæ–‡ä»¶æ•°: {len(filenames)}")
    
    # 1. éªŒè¯å‚æ•°
    if not filenames:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail={
                "error": "no_filenames",
                "message": "æœªæŒ‡å®šè¦åˆ é™¤çš„æ–‡ä»¶å"
            }
        )
    
    # 2. å¤„ç†æ¯ä¸ªæ–‡ä»¶
    details = []
    deleted = 0
    not_found = 0
    failed = 0
    
    for filename in filenames:
        try:
            # æ£€æŸ¥æ–‡æ¡£æ˜¯å¦å­˜åœ¨
            chunks_count = get_document_chunks_count(filename)
            
            if chunks_count == 0:
                logger.info(f"ğŸ“ æ–‡æ¡£ä¸å­˜åœ¨: {filename}")
                not_found += 1
                details.append(DeleteDetail(
                    filename=filename,
                    status="not_found",
                    message="æ–‡æ¡£ä¸å­˜åœ¨" if not force else "æ–‡æ¡£ä¸å­˜åœ¨ï¼ˆå¼ºåˆ¶æ¨¡å¼ï¼Œå¿½ç•¥ï¼‰"
                ))
                continue
            
            # æ‰§è¡Œåˆ é™¤
            logger.info(f"ğŸ—‘ï¸  åˆ é™¤æ–‡æ¡£: {filename}, æ–‡æœ¬å—æ•°: {chunks_count}")
            result = qdrant_client.delete(
                collection_name=COLLECTION_NAME,
                points_selector=Filter(
                    must=[
                        FieldCondition(
                            key="metadata.filename",
                            match=MatchValue(value=filename)
                        )
                    ]
                )
            )
            
            deleted += 1
            details.append(DeleteDetail(
                filename=filename,
                status="deleted",
                chunks_removed=chunks_count,
                message="åˆ é™¤æˆåŠŸ"
            ))
            logger.info(f"âœ… æ–‡æ¡£åˆ é™¤æˆåŠŸ: {filename}")
            
        except Exception as e:
            logger.error(f"âŒ åˆ é™¤æ–‡æ¡£å¤±è´¥ {filename}: {e}", exc_info=True)
            failed += 1
            details.append(DeleteDetail(
                filename=filename,
                status="failed",
                message=f"åˆ é™¤å¤±è´¥: {str(e)}"
            ))
    
    # 3. æ„å»ºå“åº”
    success = (failed == 0) and (not_found == 0 or force)
    results = {
        "deleted": deleted,
        "not_found": not_found,
        "failed": failed,
        "total_requested": len(filenames)
    }
    
    logger.info("="*60)
    logger.info(f"ğŸ“Š åˆ é™¤ç»Ÿè®¡: {results}")
    logger.info("="*60)
    
    return DeleteResponse(
        success=success,
        message="æ–‡æ¡£åˆ é™¤å®Œæˆ" if success else "éƒ¨åˆ†æ–‡æ¡£åˆ é™¤å¤±è´¥æˆ–æœªæ‰¾åˆ°",
        results=results,
        details=details
    )


@app.get("/api/documents", response_model=List[DocumentInfo], tags=["Documents"])
async def list_documents() -> List[DocumentInfo]:
    """
    åˆ—å‡ºæ‰€æœ‰å·²ä¸Šä¼ çš„æ–‡æ¡£
    
    Returns:
        æ–‡æ¡£åˆ—è¡¨ï¼ŒåŒ…å«æ–‡ä»¶åã€æ–‡æœ¬å—æ•°é‡ç­‰ä¿¡æ¯
        
    ç¤ºä¾‹ï¼š
        curl http://localhost:8080/api/documents
    """
    try:
        # è·å–æ‰€æœ‰ç‚¹çš„å…ƒæ•°æ®
        all_docs = {}
        offset = None
        
        while True:
            result = qdrant_client.scroll(
                collection_name=COLLECTION_NAME,
                limit=100,
                offset=offset,
                with_payload=True,
                with_vectors=False
            )
            
            points, next_offset = result
            
            if not points:
                break
            
            for point in points:
                filename = point.payload.get('metadata', {}).get('filename', 'unknown')
                source = point.payload.get('metadata', {}).get('source', filename)
                
                if filename not in all_docs:
                    all_docs[filename] = {
                        'filename': filename,
                        'source': source,
                        'chunks_count': 0
                    }
                all_docs[filename]['chunks_count'] += 1
            
            if next_offset is None:
                break
            offset = next_offset
        
        documents = [
            DocumentInfo(
                filename=info['filename'],
                chunks_count=info['chunks_count'],
                source=info['source']
            )
            for info in all_docs.values()
        ]
        
        logger.info(f"ğŸ“š è¿”å›æ–‡æ¡£åˆ—è¡¨ï¼Œå…± {len(documents)} ä¸ªæ–‡æ¡£")
        return documents
        
    except Exception as e:
        logger.error(f"âŒ è·å–æ–‡æ¡£åˆ—è¡¨å¤±è´¥: {e}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail={
                "error": "list_documents_failed",
                "message": f"è·å–æ–‡æ¡£åˆ—è¡¨å¤±è´¥: {str(e)}"
            }
        )


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(
        app,
        host="0.0.0.0",
        port=8000,
        log_level="info"
    )